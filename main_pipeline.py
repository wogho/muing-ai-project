import os
import subprocess
import csv
from collections import defaultdict
import numpy as np
import librosa
import soundfile as sf
import pandas as pd
import shutil
import json
import argparse
from basic_pitch.inference import predict_and_save
from basic_pitch import ICASSP_2022_MODEL_PATH

def run_full_pipeline(input_file, output_dir, chord_source):
    # --- 1. í´ë” ì´ˆê¸°í™” ---
    print(f">>> 0. ì¶œë ¥ í´ë” '{output_dir}'ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
    if os.path.exists(output_dir): shutil.rmtree(output_dir)
    os.makedirs(output_dir)

    # --- 2. íŒŒì¼ ê²½ë¡œ ì •ì˜ ---
    demucs_output_dir = os.path.join(output_dir, 'htdemucs', os.path.splitext(os.path.basename(input_file))[0])
    vocals_path = os.path.join(demucs_output_dir, 'vocals.wav'); bass_path = os.path.join(demucs_output_dir, 'bass.wav'); drums_path = os.path.join(demucs_output_dir, 'drums.wav'); other_path = os.path.join(demucs_output_dir, 'other.wav')
    melody_midi_path = os.path.join(output_dir, 'melody_output.mid'); melody_notes_csv_path = os.path.join(output_dir, 'melody_notes.csv')
    chords_csv_path = os.path.join(output_dir, 'chords_timeline.csv'); rhythm_json_path = os.path.join(output_dir, 'rhythm_info.json')
    timeline_json_path = os.path.join(output_dir, 'muing_timeline.json'); temp_chord_audio_path = os.path.join(output_dir, 'temp_chord_audio.wav')

    # --- 3. ìŠ¤í…œ ë¶„ë¦¬ ---
    print("\n>>> 1. ìŠ¤í…œ ë¶„ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    command = f"demucs \"{input_file}\" -o {output_dir}"
    try:
        subprocess.run(command, shell=True, check=True, capture_output=True, text=True)
        print(">>> ìŠ¤í…œ ë¶„ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    except subprocess.CalledProcessError as e:
        print(f"âŒ Demucs ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n--- STDERR ---\n{e.stderr}"); raise e

    # --- 4. ë©œë¡œë”” ì¶”ì¶œ ---
    print("\n>>> 2. ë©œë¡œë”” ì¶”ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    predict_and_save(
        audio_path_list=[vocals_path], model_or_model_path=ICASSP_2022_MODEL_PATH, output_directory=output_dir,
        save_midi=True, save_notes=True, sonify_midi=False, save_model_outputs=False
    )
    generated_midi = os.path.join(output_dir, os.path.splitext(os.path.basename(vocals_path))[0] + '_basic_pitch.mid')
    generated_csv = os.path.join(output_dir, os.path.splitext(os.path.basename(vocals_path))[0] + '_basic_pitch.csv')
    os.rename(generated_midi, melody_midi_path); os.rename(generated_csv, melody_notes_csv_path)
    print(f">>> ë©œë¡œë”” MIDI ë° ë…¸íŠ¸ ë°ì´í„° íŒŒì¼ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    # --- 5. ì½”ë“œ ë¶„ì„ ---
    print("\n>>> 3. ì½”ë“œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    if chord_source == 'instrumental_mix':
        print(">>> ë¶„ì„ ëŒ€ìƒ: ë°˜ì£¼ ì „ì²´ (bass + drums + other)")
        bass, sr = librosa.load(bass_path, sr=None); drums, _ = librosa.load(drums_path, sr=sr); other, _ = librosa.load(other_path, sr=sr)
        instrumental_mix = bass + drums + other; sf.write(temp_chord_audio_path, instrumental_mix, sr); chord_audio_path = temp_chord_audio_path
    else:
        chord_audio_path = input_file if chord_source == 'full_mix' else os.path.join(demucs_output_dir, f'{chord_source}.wav')
    
    temp_notes_csv_path = os.path.join(output_dir, os.path.splitext(os.path.basename(chord_audio_path))[0] + '_basic_pitch.csv')
    predict_and_save(
        audio_path_list=[chord_audio_path], model_or_model_path=ICASSP_2022_MODEL_PATH, output_directory=output_dir,
        save_midi=False, save_notes=True, sonify_midi=False, save_model_outputs=False
    )
    NOTE_NAMES = ['C','C#','D','D#','E','F','F#','G','G#','A','A#','B']; CHORD_TEMPLATES = {'maj':{0,4,7}, 'min':{0,3,7}, 'dom7':{0,4,7,10}}
    notes_in_time = defaultdict(list)
    with open(temp_notes_csv_path, 'r') as f:
        reader = csv.reader(f); next(reader)
        for row in reader: notes_in_time[int(float(row[0])/0.5)*0.5].append(int(row[2]))
    chord_progression = []
    last_chord = None
    for time_key in sorted(notes_in_time.keys()):
        pitch_classes = {note % 12 for note in notes_in_time[time_key]}
        best_match_chord, best_match_score = 'N', 0
        for root in range(12):
            for chord_type, template in CHORD_TEMPLATES.items():
                chord_notes = {(root+i)%12 for i in template}; score = len(pitch_classes.intersection(chord_notes))
                if score > best_match_score: best_match_score, best_match_chord = score,f"{NOTE_NAMES[root]}:{chord_type}"
        if best_match_score >= 2 and best_match_chord != last_chord: chord_progression.append({'time':f"{time_key:.2f}",'chord':best_match_chord}); last_chord = best_match_chord
    pd.DataFrame(chord_progression).to_csv(chords_csv_path, index=False)
    print(">>> ì½”ë“œ ì§„í–‰ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

    # --- 6. ë¦¬ë“¬ ë¶„ì„ ---
    print("\n>>> 6. ë¦¬ë“¬(BPM ë° ë°•ì) ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    y, sr = librosa.load(input_file, sr=None); tempo, beats = librosa.beat.beat_track(y=y, sr=sr)
    beat_times = librosa.frames_to_time(beats, sr=sr)
    rhythm_data = {'bpm': round(float(tempo), 2), 'beat_times': [round(t, 2) for t in beat_times]}
    with open(rhythm_json_path, 'w') as f: json.dump(rhythm_data, f, indent=2)
    print(">>> ë¦¬ë“¬ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

    # --- 7. ë°ì´í„° í†µí•© ---
    print("\n>>> 7. ëª¨ë“  ë¶„ì„ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ íƒ€ì„ë¼ì¸ìœ¼ë¡œ í†µí•©í•©ë‹ˆë‹¤.")
    all_events = []
    with open(melody_notes_csv_path, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            # --- âœ¨ ìš”ì²­í•˜ì‹  ìˆ˜ì •ì‚¬í•­ + ë””ë²„ê·¸ ì½”ë“œ ëª¨ë‘ ì ìš© âœ¨ ---
            
            # 1. ë””ë²„ê¹…ì„ ìœ„í•´ í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ rowì˜ ë‚´ìš©ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
            print(f"DEBUG: í˜„ì¬ rowì˜ ë‚´ìš©: {row}") 

            # 2. 'velocity_midi' í‚¤ê°€ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ê¸°ë³¸ê°’(64)ì„ ì‚¬ìš©í•˜ì—¬ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
            velocity = int(row.get('velocity_midi', 64))
            
            # 3. íƒ€ì„ë¼ì¸ ì´ë²¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì— ë…¸íŠ¸ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
            all_events.append({
                'time': round(float(row['start_time_s']), 2),
                'type': 'note',
                'pitch': int(row['pitch_midi']),
                'duration': round(float(row['end_time_s']) - float(row['start_time_s']), 2),
                'velocity': velocity
            })
            # --- âœ¨ ì ìš© ë âœ¨ ---
            
    with open(chords_csv_path, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader: all_events.append({'time':round(float(row['time']),2),'type':'chord','value':row['chord']})
    for beat_time in rhythm_data['beat_times']: all_events.append({'time':beat_time,'type':'beat'})
    all_events.sort(key=lambda x: x['time'])
    final_timeline_data = { 'bpm': rhythm_data['bpm'], 'events': all_events }
    with open(timeline_json_path, 'w') as f: json.dump(final_timeline_data, f, indent=2, ensure_ascii=False)
    print(f">>> ìµœì¢… í†µí•© ë°ì´í„° íŒŒì¼ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤: '{timeline_json_path}'")

    # --- 8. ì„ì‹œ íŒŒì¼ ì‚­ì œ ---
    if os.path.exists(temp_chord_audio_path): os.remove(temp_chord_audio_path)
    if os.path.exists(temp_notes_csv_path): os.remove(temp_notes_csv_path)
    
    return timeline_json_path

def main():
    parser = argparse.ArgumentParser(description="Muing AI: ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ë©œë¡œë””, ì½”ë“œ, ë¦¬ë“¬ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.")
    parser.add_argument("input_file", type=str, help="ë¶„ì„í•  ì˜¤ë””ì˜¤ íŒŒì¼ì˜ ê²½ë¡œ")
    parser.add_argument("-o", "--output_dir", type=str, default="output", help="ê²°ê³¼ë¬¼ì´ ì €ì¥ë  í´ë”")
    parser.add_argument("-s", "--source", type=str, default="instrumental_mix", help="ì½”ë“œ ë¶„ì„ì— ì‚¬ìš©í•  ì†ŒìŠ¤")
    args = parser.parse_args()
    timeline_json_path = run_full_pipeline(args.input_file, args.output_dir, args.source)
    print(f"\nğŸ‘‘ ìµœì¢… í†µí•© íƒ€ì„ë¼ì¸: '{timeline_json_path}'"); print("\nëª¨ë“  íŒŒì´í”„ë¼ì¸ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()
